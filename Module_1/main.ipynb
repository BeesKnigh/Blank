{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выполнение 1 задания, 1 подпункт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Requirement already satisfied: six>=1.5 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, numpy, pandas\n",
      "Successfully installed numpy-2.1.3 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'T:\\code_t\\Blanks\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "text_dir = 'data/path/to/text'\n",
    "annotation_dir = 'data/path/to/annotation'\n",
    "tags_dir = 'data/path/to/tags'\n",
    "\n",
    "# список файлов в каждой папке\n",
    "text_files = sorted(os.listdir(text_dir))\n",
    "annotation_files = sorted(os.listdir(annotation_dir))\n",
    "tags_files = sorted(os.listdir(tags_dir))\n",
    "\n",
    "# Проверка на совпадение кол-ва файлов.\n",
    "assert len(text_files) == len(annotation_files) == len(tags_files), \"Количество файлов в папках не совпадает\"\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'text_file': text_files,\n",
    "    'annotation_file': annotation_files,\n",
    "    'tags_file': tags_files\n",
    "})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на \"Заспамленность\" 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = ['купить', 'бесплатно', 'срочно', 'акция', 'выиграй']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spam_score(text):\n",
    "    text = text.lower()\n",
    "    count = sum(text.count(word) for word in spam_words)\n",
    "    return count\n",
    "\n",
    "# Функция для подсчёта символов с и без пробелов\n",
    "def count_chars(text):\n",
    "    with_spaces = len(text)\n",
    "    without_spaces = len(text.replace(\" \", \"\"))\n",
    "    return with_spaces, without_spaces\n",
    "\n",
    "# Функция для подсчёта ключевых слов\n",
    "def count_keywords(tags):\n",
    "    keywords = tags.split(',')  # Предполагаю, что ключевые слова разделены запятыми\n",
    "    return len([kw.strip() for kw in keywords if kw.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Списки для хранения значений\n",
    "spam_scores = []\n",
    "annotation_lengths = []\n",
    "keywords_counts = []\n",
    "chars_with_spaces = []\n",
    "chars_without_spaces = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in data.iterrows():\n",
    "    with open(os.path.join(text_dir, row['text_file']), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    with open(os.path.join(annotation_dir, row['annotation_file']), 'r', encoding='utf-8') as f:\n",
    "        annotation = f.read()\n",
    "    \n",
    "    with open(os.path.join(tags_dir, row['tags_file']), 'r', encoding='utf-8') as f:\n",
    "        tags = f.read()\n",
    "        \n",
    "    spam_scores.append(calculate_spam_score(text))\n",
    "    annotation_lengths.append(len(annotation))\n",
    "    keywords_counts.append(count_keywords(tags))\n",
    "    w_spaces, wo_spaces = count_chars(text)\n",
    "    chars_with_spaces.append(w_spaces)\n",
    "    chars_without_spaces.append(wo_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датафрейма под вычисленные параметры из функций\n",
    "\n",
    "data['spam_score'] = spam_scores\n",
    "data['annotation_length'] = annotation_lengths\n",
    "data['keywords_count'] = keywords_counts\n",
    "data['chars_with_spaces'] = chars_with_spaces\n",
    "data['chars_without_spaces'] = chars_without_spaces\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистический анализ 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in t:\\code_t\\blanks\\venv\\lib\\site-packages (3.9.2)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: pillow>=8 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.23 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in t:\\code_t\\blanks\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'T:\\code_t\\Blanks\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def plot_distribution(column, title):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(data[column], bins=30, kde=True)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution('spam_score', 'Распределение заспамленности')\n",
    "plot_distribution('annotation_length', 'Распределение длины аннотации')\n",
    "plot_distribution('keywords_count', 'Распределение количества ключевых слов')\n",
    "plot_distribution('chars_with_spaces', 'Распределение количества символов с пробелами')\n",
    "plot_distribution('chars_without_spaces', 'Распределение количества символов без пробелов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее надо проводить очистку датасета и сохранение его, но у меня нету данных, поэтому просто вставил заготовку, которая была. (модифицировал ее под задачу с помощью ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчитаем медиану и 75-й перцентиль для каждого параметра\n",
    "thresholds = {\n",
    "    'spam_score': data['spam_score'].median(),\n",
    "    'annotation_length': data['annotation_length'].median(),\n",
    "    'keywords_count': data['keywords_count'].median(),\n",
    "    'chars_with_spaces': data['chars_with_spaces'].median(),\n",
    "    'chars_without_spaces': data['chars_without_spaces'].median()\n",
    "}\n",
    "\n",
    "print(\"Пороговые значения (медиана):\")\n",
    "for key, value in thresholds.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчитаем 75-й перцентиль\n",
    "for key in thresholds.keys():\n",
    "    thresholds[key] = data[key].quantile(0.75)\n",
    "\n",
    "print(\"\\nПороговые значения (75-й перцентиль):\")\n",
    "for key, value in thresholds.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для фильтрации\n",
    "def filter_data(df, thresholds):\n",
    "    filtered_df = df[\n",
    "        (df['spam_score'] <= thresholds['spam_score']) &\n",
    "        (df['annotation_length'] >= thresholds['annotation_length']) &\n",
    "        (df['keywords_count'] >= thresholds['keywords_count']) &\n",
    "        (df['chars_with_spaces'] >= thresholds['chars_with_spaces']) &\n",
    "        (df['chars_without_spaces'] >= thresholds['chars_without_spaces'])\n",
    "    ]\n",
    "    return filtered_df\n",
    "\n",
    "# Применяем фильтрацию\n",
    "filtered_data = filter_data(data, thresholds)\n",
    "\n",
    "print(f\"Количество статей до фильтрации: {len(data)}\")\n",
    "print(f\"Количество статей после фильтрации: {len(filtered_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим новую папку для отфильтрованных данных\n",
    "filtered_text_dir = 'path/to/filtered_text'\n",
    "filtered_annotation_dir = 'path/to/filtered_annotation'\n",
    "filtered_tags_dir = 'path/to/filtered_tags'\n",
    "\n",
    "os.makedirs(filtered_text_dir, exist_ok=True)\n",
    "os.makedirs(filtered_annotation_dir, exist_ok=True)\n",
    "os.makedirs(filtered_tags_dir, exist_ok=True)\n",
    "\n",
    "# Копируем файлы\n",
    "import shutil\n",
    "\n",
    "for idx, row in filtered_data.iterrows():\n",
    "    shutil.copy(os.path.join(text_dir, row['text_file']), os.path.join(filtered_text_dir, row['text_file']))\n",
    "    shutil.copy(os.path.join(annotation_dir, row['annotation_file']), os.path.join(filtered_annotation_dir, row['annotation_file']))\n",
    "    shutil.copy(os.path.join(tags_dir, row['tags_file']), os.path.join(filtered_tags_dir, row['tags_file']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv('filtered_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
